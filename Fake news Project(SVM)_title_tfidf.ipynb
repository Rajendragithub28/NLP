{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1213834d",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe2 in position 91756: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 145\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m#==============================================For Training (Only one time)==============================\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m#nltk.download('stopwords')\u001b[39;00m\n\u001b[0;32m    141\u001b[0m Path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 145\u001b[0m Train_Model(Path)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m#==============================================For Prediction==============================\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# Load the model from disk\u001b[39;00m\n\u001b[0;32m    150\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel2_SVM_title2.sav\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[2], line 76\u001b[0m, in \u001b[0;36mTrain_Model\u001b[1;34m(Path)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mTrain_Model\u001b[39m(Path):\n\u001b[1;32m---> 76\u001b[0m     X_text1, X_title1, y1 \u001b[38;5;241m=\u001b[39m datasets(Path)\n\u001b[0;32m     77\u001b[0m     corpus1_title \u001b[38;5;241m=\u001b[39m simplify(X_title1, y1)\n\u001b[0;32m     78\u001b[0m     X_train_title1, X_test_title1, y_train_title1, y_test_title1 \u001b[38;5;241m=\u001b[39m train_test_split(corpus1_title, y1, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m) \n",
      "Cell \u001b[1;32mIn[2], line 20\u001b[0m, in \u001b[0;36mdatasets\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdatasets\u001b[39m(path):\n\u001b[1;32m---> 20\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path)\n\u001b[0;32m     21\u001b[0m     X_text \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     22\u001b[0m     X_title \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mE:\\Python\\software\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Python\\software\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Python\\software\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mE:\\Python\\software\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32mE:\\Python\\software\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1779\u001b[0m         nrows\n\u001b[0;32m   1780\u001b[0m     )\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mE:\\Python\\software\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mE:\\Python\\software\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mE:\\Python\\software\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mE:\\Python\\software\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mE:\\Python\\software\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1965\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe2 in position 91756: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "import json\n",
    "\n",
    "def datasets(path):\n",
    "    df = pd.read_csv(path)\n",
    "    X_text = df['text'].values\n",
    "    X_title = df['title'].values\n",
    "    y = df['label'].values\n",
    "    return X_text[:1000], X_title[:1000], y[:1000]\n",
    "\n",
    "\n",
    "\n",
    "def checking_Unbiasity(y):\n",
    "    plt.figure(figsize=(7,6))\n",
    "    labels = 'Fake', 'Real'\n",
    "    a=0\n",
    "    b=0\n",
    "    for i in y:\n",
    "      if i==0:\n",
    "        a+=1\n",
    "      else:\n",
    "        b+=1\n",
    "    sizes = [b,a]\n",
    "    colors = ['lightcoral', 'teal']\n",
    "    explode = (0.1, 0) \n",
    "    plt.rcParams['font.size'] = 18\n",
    "    plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "    autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def simplify(df,y):\n",
    "  corpus = []\n",
    "  for i in range(0, y.size):\n",
    "    review = re.sub('[^a-zA-Z0-9]', ' ', str(df[i]))\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    all_stopwords.remove('not')\n",
    "    review = [ps.stem(word)\n",
    " for word in review if not word in set(all_stopwords)]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "  return corpus\n",
    "\n",
    "\n",
    "def Extract_TFIDF_Features(TfV,q):\n",
    "    return TfV.transform(q).toarray()\n",
    "    \n",
    "def Extract_Features_Training(X):\n",
    "    # converting the textual data to numerical data\n",
    "    TfV = TfidfVectorizer()\n",
    "    TfV .fit(X)\n",
    "    X = TfV .transform(X)\n",
    "    return X,TfV\n",
    "\n",
    "def Train_Model(Path):\n",
    "    X_text1, X_title1, y1 = datasets(Path)\n",
    "    corpus1_title = simplify(X_title1, y1)\n",
    "    X_train_title1, X_test_title1, y_train_title1, y_test_title1 = train_test_split(corpus1_title, y1, test_size=0.3, random_state=42) \n",
    "    X_train_title1,TfV = Extract_Features_Training(X_train_title1)\n",
    "    X_test_title1 = Extract_TFIDF_Features(TfV,X_test_title1)\n",
    "    clf=train(X_train_title1,y_train_title1)\n",
    "    report={\"Data Url:\":\"https://www.kaggle.com/c/fake-news/data\"}\n",
    "    report=model_eval(report, clf, X_test_title1, y_test_title1)\n",
    "    # save the model to disk\n",
    "    filename = 'Model2_SVM_title2.sav'\n",
    "    joblib.dump(clf, filename)\n",
    "    # save the CountVectorizer to disk\n",
    "    filename_TfV = 'TfV_2.sav'\n",
    "    joblib.dump(TfV, filename_TfV)\n",
    "\n",
    "    # save report in json file\n",
    "    with open('Model2_SVM_Report_title2.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, ensure_ascii=False, indent=4)\n",
    "    print(\"Training completed successfully.........\")\n",
    "    \n",
    "    \n",
    "def train(x_train, y_train):\n",
    "    clf = SVC(max_iter=10)\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf\n",
    "    \n",
    "def model_eval(report, clf, X_test, y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, fscore, support = score(y_test, y_pred,average='weighted')\n",
    "    \n",
    "    print(report)\n",
    "    report[\"accuracy\"]=acc\n",
    "    report[\"precision\"]=precision\n",
    "    report[\"recall\"]=recall\n",
    "    report[\"F-Score\"]=fscore\n",
    "    return report\n",
    "\n",
    "\n",
    "def predict(clf,x_query):\n",
    "    y_pred=clf.predict(x_query)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "\n",
    "def plotting_accuracies(test_accuracy_text,test_accuracy_title,test_accuracy_tt):\n",
    "  l=[test_accuracy_text,test_accuracy_title,test_accuracy_tt]\n",
    "  l2=['Text','Title','Text+Title']\n",
    "  d1={'Accuracy':l,'Variation':l2}\n",
    "  d1=pd.DataFrame(d1)\n",
    "  \n",
    "\n",
    "  fig = px.bar(d1, y='Accuracy', x='Variation', text='Accuracy',color='Variation',title='Accuracy Analysis on train.csv')\n",
    "  fig.update_traces(texttemplate='%{text}', textposition='outside')\n",
    "  fig.show()\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    Label={1: \"Fake\",\n",
    "           0: \"Genuine\"}\n",
    "    \n",
    "    #==============================================For Training (Only one time)==============================\n",
    "    #nltk.download('stopwords')\n",
    "    Path=\"train.csv\"\n",
    "\n",
    "   \n",
    "\n",
    "    Train_Model(Path)\n",
    "  \n",
    "\n",
    "    #==============================================For Prediction==============================\n",
    "    # Load the model from disk\n",
    "    filename = 'Model2_SVM_title2.sav'\n",
    "    clf=joblib.load(filename)\n",
    "    # Load the CountVectorizer from disk\n",
    "    filename_TfV = 'TfV_2.sav'\n",
    "    TfV=joblib.load(filename_TfV)\n",
    "    # Load the model report from disk\n",
    "\n",
    " \n",
    "    # Opening JSON file\n",
    "    f = open('Model2_SVM_Report_title2.json') \n",
    "    report = json.load(f)\n",
    "        \n",
    "    \n",
    "    q=['Manas is a good boy']\n",
    "    f=Extract_TFIDF_Features(TfV,q)       \n",
    "    r=predict(clf,f)\n",
    "\n",
    "    report[\"Input\"]=q[0]\n",
    "    report[\"Prediction\"]=Label[r[0]]\n",
    "    \n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c2f07f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
